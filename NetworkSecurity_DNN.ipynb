{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "799qqmyAd1HO",
        "outputId": "686461d1-534c-4931-c598-2b2a983dcb4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "# #@title Random Code Test\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# # list of text documents\n",
        "# text = \"The quick brown fox#  jumped! over@ the lazy. dog?\" + \"\\n\" + \"i love cookies\" + \" Anh likes chocolate\"\n",
        "# print(text)\n",
        "# text = [text]\n",
        "# print(text[0])\n",
        "# # create the transform\n",
        "# # vectorizer = CountVectorizer(token_pattern=r\"[\\s\\S]\")\n",
        "# vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w\\w+\\b|\\S*[^\\w\\s]\\S*|\\s+|\\r\\n\")\n",
        "# # tokenize and build vocab\n",
        "# vectorizer.fit(text)\n",
        "# # summarize\n",
        "# print(vectorizer.vocabulary_)\n",
        "# # encode document\n",
        "# vector = vectorizer.transform(text)\n",
        "# # summarize encoded vector\n",
        "# print(vector.shape)\n",
        "# print(type(vector))\n",
        "# print(vector.toarray())\n",
        "\n",
        "oned_array = torch.as_tensor([0,1,1])\n",
        "print(oned_array.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFboelAsCvyE",
        "outputId": "f6272621-e472-499d-ded8-517ceb5ac618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gmpy2\n",
            "  Downloading gmpy2-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: gmpy2\n",
            "Successfully installed gmpy2-2.1.2\n"
          ]
        }
      ],
      "source": [
        "pip install gmpy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKQ5bWni-rXh",
        "outputId": "66a87d3c-6eea-4fae-932e-bf6f309ae65d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting essential-generators\n",
            "  Downloading essential_generators-1.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: essential-generators\n",
            "Successfully installed essential-generators-1.0\n"
          ]
        }
      ],
      "source": [
        "pip install essential-generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd2VNTwNwO_n",
        "outputId": "a868e6cc-e603-45ef-ba2c-55c8ec26d4d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitstring\n",
            "  Downloading bitstring-3.1.9-py3-none-any.whl (38 kB)\n",
            "Installing collected packages: bitstring\n",
            "Successfully installed bitstring-3.1.9\n"
          ]
        }
      ],
      "source": [
        "pip install bitstring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDezfwEekZR4"
      },
      "outputs": [],
      "source": [
        "#@title Import\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pickle as pkl\n",
        "from numpy import linalg as LA \n",
        "from essential_generators import DocumentGenerator\n",
        "import hashlib\n",
        "from bitstring import BitArray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L33l0ui_mxYL"
      },
      "outputs": [],
      "source": [
        "#@title Constant\n",
        "INPUT_DIM = 1024\n",
        "OUT_DIM = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCXjzp80feHb"
      },
      "outputs": [],
      "source": [
        "#@title Preprocess\n",
        "def preprocess(file, isSentence=True):\n",
        "    if isSentence:\n",
        "        send_file = [file]\n",
        "    else:\n",
        "        send_file = open(file, \"r\").readlines()\n",
        "        send_file = ' '.join(send_file)\n",
        "        send_file = [send_file]\n",
        "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w\\w+\\b|\\S*[^\\w\\s]\\S*|\\s+|\\r\\n\")\n",
        "    # tokenize and build vocab\n",
        "    vectorizer.fit(send_file)\n",
        "    # encode document\n",
        "    vector = vectorizer.transform(send_file)\n",
        "    # summarize encoded vector\n",
        "    vector_shape = vector.shape\n",
        "    doc_feat = torch.tensor(vector.toarray(), dtype=torch.float)\n",
        "    #Add padding\n",
        "    if vector_shape[1] < INPUT_DIM:\n",
        "      pad_size = INPUT_DIM - vector_shape[1]\n",
        "      padding = (0, pad_size)\n",
        "      doc_feat = F.pad(doc_feat, padding, \"constant\", 0)\n",
        "    elif vector_shape[1] < INPUT_DIM:\n",
        "      pad_size = vector_shape[1] % INPUT_DIM\n",
        "      padding = (0, pad_size)\n",
        "      doc_feat = F.pad(doc_feat, padding, \"constant\", 0)\n",
        "      num_chunks = doc_feat.shape[1]/INPUT_DIM\n",
        "      doc_feat = doc_feat.chunk(num_chunks)\n",
        "    assert doc_feat.shape[1] == INPUT_DIM, f\"document input dim does not match. 1024 expected, got: {doc_feat.shape[1]}\"\n",
        "    return doc_feat\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg4AeUeQg1EJ"
      },
      "outputs": [],
      "source": [
        "#@title HashNN\n",
        "\n",
        "class HashNN(nn.Module):\n",
        "    def __init__(self, out_size):\n",
        "        super(HashNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512,512)\n",
        "        self.fc3 = nn.Linear(512, out_size)\n",
        "        self.sequential = nn.Sequential(self.fc1,\n",
        "                                        nn.ReLU(),\n",
        "                                        self.fc2,\n",
        "                                        nn.ReLU(),\n",
        "                                        self.fc3)\n",
        "    def forward(self, x):\n",
        "        x = self.sequential(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwZH3ll_rdZa"
      },
      "outputs": [],
      "source": [
        "x_train = np.zeros((10000, INPUT_DIM))\n",
        "x_target = np.zeros(10000)\n",
        "gen = DocumentGenerator() \n",
        "for i in range(10000):\n",
        "    send_file = gen.sentence()\n",
        "    doc_feat = preprocess(send_file)\n",
        "    x_train[i] = doc_feat\n",
        "    h = hashlib.new('sha256')\n",
        "    hash = send_file.encode('utf-8')\n",
        "    target = hashlib.sha256(hash).digest()[:4]\n",
        "    bin = int.from_bytes(target, \"big\")\n",
        "    x_target[i] = bin\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5Kq4Yg5wfWM"
      },
      "outputs": [],
      "source": [
        "#@title Model training\n",
        "hash_model = HashNN(1)\n",
        "epochs = np.random.randint(100, 500)\n",
        "optimizer = optim.SGD(hash_model.parameters(), lr=1e-5)\n",
        "criterion = nn.MSELoss()\n",
        "for i in range(1000):\n",
        "  idx = np.random.permutation(len(x_target))\n",
        "  batch_feat = torch.from_numpy(x_train[idx]).to(dtype=torch.float)\n",
        "  batch_target = torch.from_numpy(x_target[idx]).to(dtype=torch.float)\n",
        "  optimizer.zero_grad()\n",
        "  output = hash_model(batch_feat)\n",
        "  loss = criterion(output, batch_target)\n",
        "  loss.backward()\n",
        "  torch.nn.utils.clip_grad_norm(hash_model.parameters(), max_norm=15)\n",
        "  optimizer.step()\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fiNcacbAHqD"
      },
      "outputs": [],
      "source": [
        "#@title Save model\n",
        "for name, param in hash_model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)\n",
        "torch.save(hash_model.state_dict(), \"/content/Model_1.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyieYFpzq7ps"
      },
      "outputs": [],
      "source": [
        "#@title Convert to Binary\n",
        "def convert_to_binary(vector):\n",
        "  vector_tensor = torch.as_tensor(vector)\n",
        "  sign = torch.sign(vector_tensor)\n",
        "  binary_output = torch.relu(sign)  \n",
        "  return binary_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKXP2IWxyT1e"
      },
      "outputs": [],
      "source": [
        "#@title Postprocess Dimensionality Reduction\n",
        "'''\n",
        "Use dimensionality reduction\n",
        "to reduce a bit_len x chunk_size \n",
        "matrix to a single bit_len vector \n",
        "while retaining as much of the information\n",
        "from the digest as possible\n",
        "'''\n",
        "def reduce_digest(D): \n",
        "  # Convert tensy to a numpy vector\n",
        "  D = D.detach().numpy()\n",
        "  assert D.shape[0] % OUT_DIM == 0\n",
        "  # Convert to bit_len x chunk_size numpy array\n",
        "  temp = np.zeros((OUT_DIM, int(D.shape[0]/OUT_DIM)))\n",
        "  for i in range(temp.shape[1]):\n",
        "    for j in range(OUT_DIM):\n",
        "      temp[j][i] = D[(i * OUT_DIM) + j]\n",
        "  D = temp\n",
        "  # Center the data \n",
        "  c = np.outer(np.ones(D.shape[1]), D.mean(0))\n",
        "  D = D - c \n",
        "  # Get the top right singular vector\n",
        "  U, S, Vt = LA.svd(D, full_matrices=True)\n",
        "  Vt = Vt[:1, :].T  \n",
        "  # Reconstruct the data\n",
        "  D = D + c \n",
        "  # Project down to 1-dim\n",
        "  return D @ Vt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Rjxvw4HPtm6a"
      },
      "outputs": [],
      "source": [
        "#@title Running Preprocess and Model output\n",
        "collision = 0\n",
        "hash_outputs = set()\n",
        "gen = DocumentGenerator() \n",
        "for _ in range(10):\n",
        "    send_file = gen.sentence()\n",
        "    doc_feat = preprocess(send_file)\n",
        "    print(doc_feat)\n",
        "    hash_output = hash_model(doc_feat).detach().item()\n",
        "    print(hash_output)\n",
        "    '0x{0:08X}'.format(int(hash_output))\n",
        "    # hash_output = torch.squeeze(hash_output.view(1,-1))\n",
        "    # hash_output = torch.exp(hash_output)\n",
        "    # if hash_output.shape[0] > OUT_DIM:\n",
        "    #     hash_output = reduce_digest(hash_output)\n",
        "    #     hash_output = torch.from_numpy(hash_output)\n",
        "    print(\"Hash output: \", hash_output)\n",
        "    # b = [\" \".join(item) for item in binary_out.astype(str) ]\n",
        "    # b = \"\".join(b)\n",
        "    # bytes_hash = int(b, 2).to_bytes((len(b) + 7) // 8, 'big')\n",
        "    # print(bytes_hash)\n",
        "    if bytes_hash in hash_outputs:\n",
        "      print(collision)\n",
        "    else:\n",
        "      hash_outputs.add(bytes_hash)\n",
        "    collision +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPug0QfVMEcF"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJzDMzHbPbAB"
      },
      "outputs": [],
      "source": [
        "!pip install inltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8btEChlPHRU"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfAkI0YdPOAB"
      },
      "outputs": [],
      "source": [
        "from inltk.inltk import setup\n",
        "setup('en') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWbhl7urQRHo"
      },
      "outputs": [],
      "source": [
        "from inltk.inltk import get_similar_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U216JfzfPRjL"
      },
      "outputs": [],
      "source": [
        "# sample sentence\n",
        "sentence1 = \"The Quick Brown Fox Jumps Over The Lazy Dog\"\n",
        "sentence2 = \"I eat rice\"\n",
        "sentence3 = \"She sells seashells\"\n",
        "sentence4 = \"We love icecream\"\n",
        "sentence5 = \"Greatest Snow on Earth\"\n",
        "sentence6 = \"We love the U\"\n",
        "# number of similar sentences to be generated\n",
        "no_of_variants = 5                               # i want 6*5 similar sentences\n",
        "\n",
        "# code of language \n",
        "code_of_language = \"en\"                                       # \"en\" is a code to represent english language\n",
        "\n",
        "# where degree_of_aug is roughly the percentage of sentence you want to augment, with a default value of 0.1\n",
        "degree_of_aug = 0.3\n",
        "\n",
        "result1 = get_similar_sentences(sentence1, no_of_variants, code_of_language, degree_of_aug)\n",
        "result2 = get_similar_sentences(sentence2, no_of_variants, code_of_language, degree_of_aug)\n",
        "result3 = get_similar_sentences(sentence3, no_of_variants, code_of_language, degree_of_aug)\n",
        "result4 = get_similar_sentences(sentence4, no_of_variants, code_of_language, degree_of_aug)\n",
        "result5 = get_similar_sentences(sentence5, no_of_variants, code_of_language, degree_of_aug)\n",
        "result6 = get_similar_sentences(sentence6, no_of_variants, code_of_language, degree_of_aug)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6y-TX6gQc27"
      },
      "outputs": [],
      "source": [
        "for sentence1 in result1:\n",
        "  print(sentence1)\n",
        "for sentence2 in result2:\n",
        "  print(sentence2)\n",
        "for sentence3 in result3:\n",
        "  print(sentence3)\n",
        "for sentence4 in result4:\n",
        "  print(sentence4)\n",
        "for sentence5 in result5:\n",
        "  print(sentence5)\n",
        "for sentence6 in result6:\n",
        "  print(sentence6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doxZIh0Pz8Gp",
        "outputId": "0f7045f4-248b-4843-d716-2a62761f4707"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B20Ngktz0xhf",
        "outputId": "b5694abd-f5ef-45b2-de54-4f4a9c8098c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"/content/drive/MyDrive/ltrc_yahoo (2).zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zipping:\n",
        "  zipping.extractall(\"/content/Fair-PGRank/yahoo\")\n",
        "  print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUcU3NT_0_Cq",
        "outputId": "9db6b259-9825-44f6-e33d-ab24ba55052f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(438139, 699)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_svmlight_file\n",
        "\n",
        "x_train, y_train, query_ids_train = load_svmlight_file(\"/content/Fair-PGRank/yahoo/ltrc_yahoo/set1.train.txt\", query_id=True)\n",
        "x_train = x_train.toarray()\n",
        "x_train = np.unique(x_train, axis=0)\n",
        "print(x_train.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NetworkSecurity.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}